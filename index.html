<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="HRP: Human Affordances for Robotic Pre-Training">
  <meta name="keywords" content="Robotic Manipulation">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>HRP</title>

  <script async src="https://www.googletagmanager.com/gtag/js?id=G-FV4ZJ9PVSV"></script>  
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'G-FV4ZJ9PVSV');
  </script>

  <script>

    function updateInteractive() {
      var task = document.getElementById("interative-menu").value;

      console.log("interactive", task)

      var video = document.getElementById("interactive-video1");
      video.src = "assets/h3_vids/goal" + task + ".mp4";
      video.play();

      var video = document.getElementById("interactive-video2");
      video.src = "assets/h3_vids/sketch" + task + ".mp4";
      video.play();

    }

    function updateInteractiveSemantic() {
      var task = document.getElementById("interative-semantic-menu").value;

      console.log("interactive", task)

      var video = document.getElementById("interactive-video-semantic-1");
      video.src = "assets/h4_vids/" + task + "/semantic.mp4";
      video.play();

      var video = document.getElementById("interactive-video-semantic-2");
      video.src = "assets/h4_vids/" + task + "/spatial.mp4";
      video.play();

    }



  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body onload="updateInteractive();">

<section class="hero">
  <div class="hero-body">
    <div class="container is-fullhd">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">HRP: Human Affordances for Robotic Pre-Training</h1>
          <div class="is-size-3">Published in Robotics: Science and Systems, 2024</div>
          <br>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a target="_blank" href="http://mohanksriram.com/">Mohan Kumar Srirama,</a>
              <a target="_blank" href="https://sudeepdasari.github.io/">Sudeep Dasari,</a><sup>*</sup>
              <a target="_blank" href="https://shikharbahl.github.io/">Shikhar Bahl,</a><sup>*</sup>
              <a target="_blank" href="http://www.cs.cmu.edu/~abhinavg/">Abhinav Gupta</a><sup>*</sup>
            </span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup><font size="-0.4">*</sup>Equal advising</font></span><br>
            <span class="author-block"><sup></sup>Carnegie Mellon University</span>
          </div>

          <!-- <img src="./assets/logos/cmu.png" width="8%" style="margin-right: 35px;"></img> -->


          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- TODO Paper -->
              <span class="link-block">
                <a href="./resources/paper.pdf"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="fa fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>
              <!-- TODO Arxiv -->
              <span class="link-block">
                <a href="https://arxiv.org/abs/2310.09289"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
              <!-- TODO Twitter -->
              <span class="link-block">
                <a href="https://x.com/SudeepDasari/status/1713917194097008776?s=20"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-twitter"></i>
                  </span>
                  <span>Twitter</span>
                </a>
              </span>
              <!-- TODO Code -->
              <span class="link-block">
                <a href="https://github.com/SudeepDasari/data4robotics"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span>
              <!-- TODO Models-->
              <span class="link-block">
                <a href="https://cmu.box.com/s/rrlrp5g6ynk03io4rj9uzf6nik5urfl6"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-eye"></i>
                  </span>
                  <span>Models</span>
                  </a>
            </div>

        </div>
      </div>
    </div>
  </div>
</section>


<section class="hero teaser">
  <div class="container is-fullhd">
    <div class="hero-body">
      <div class="container">
        <div class="columns is-vcentered  is-centered">
          <video id="teaser" autoplay muted loop height="70%" width="70%">
            <source src="./media/videos/hrp_task_teaser.mp4"
                    type="video/mp4">
          </video>
          <!-- <img src="./media/videos/hrp_task_teaser.mp4" style="width: 60%; height: auto;"/> -->
          </br>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="hero is-light is-small">
  <div class="hero-body">
    <div class="container is-two-thirds">
      <div class="columns is-centered has-text-centered">
        <div class="column is-two-thirds">
          <h2 class="title is-3">Abstract</h2>
        </div>
    </div>
  
    <div class="columns is-centered has-text-centered">
      <div class="column is-two-thirds ">
        <div class="content has-text-justified">
          In order to generalize to various tasks in the wild,
          robotic agents will need a suitable representation (i.e., vision
          network) that enables the robot to predict optimal actions
          given high dimensional vision inputs. However, learning such
          a representation requires an extreme amount of diverse training
          data, which is prohibitively expensive to collect on a real robot.
          How can we overcome this problem? Instead of collecting more
          robot data, this paper proposes using internet-scale, human
          videos to extract “affordances,” both at the environment and
          agent level, and distill them into a pre-trained representation.
          We present a simple framework for pre-training representations
          on hand, object, and contact “affordance labels” that highlight
          relevant objects in images and how to interact with them. These
          affordances are automatically extracted from human video data
          (with the help of off-the-shelf computer vision modules) and
          used to fine-tune existing representations. Our approach can
          efficiently fine-tune any existing representation, and results in
          models with stronger downstream robotic performance across the
          board. We experimentally demonstrate (using 3000+ robot trials)
          that this affordance pre-training scheme boosts performance
          by a minimum of 15% on 5 real-world tasks, which consider
          three diverse robot morphologies (including a dexterous hand).
          Unlike prior works in the space, these representations improve
          performance across 3 different camera views. Quantitatively, we
          find that our approach leads to higher levels of generalization in
          out-of-distribution settings.
          </div>
      </div>
      </div>
    </div>
    </div>
  </div>
</section>

<section class="hero teaser">
  <div class="container is-max-widescreen">

    <!-- <div class="rows"> -->
      <div class="rows">
        <div class="row is-full-width">
          <div class="column" style="text-align: center;">
          <h2 class="title is-3">HRP: Model</span></h2>
          </div>

      <div class="columns is-centered">
        <div class="column is-full" style="text-align: center;">
          <img src="media/figures/method.png" alt="Image description" width="80%">
          <br>
          <br>
          <div class="columns is-centered has-text-centered">
            <div class="column is-two-thirds">
              <div class="content has-text-justified interpolation-panel">
                <!-- <div class="content has-text-justified"> -->
                <p style="text-align: center;font-size: 18px"> 
                  HRP fine-tunes a pre-trained encoder to predict three classes of human affordance labels via L2 regression. Specifically,
                  the network must predict future contact points, human hand poses, and the target object given an input frame from the
                  video stream. These affordance labels are mined autonomously from a human video dataset [33] using off-the-shelf vision
                  detectors [81]. HRP representations are then fine-tuned to solve downstream manipulation tasks via behavior cloning.
                </p>
            </div>
          </div>
        </div>
        <br>
          
        </div>
      </div>

        </div>
      </div>
    </div>
    </div>
  </div>
</section>

<section class="hero teaser">
  <div class="container is-max-widescreen">

    <!-- <div class="rows"> -->
      <div class="rows">
        <div class="row is-full-width">
          <div class="column" style="text-align: center;">
          <h2 class="title is-3">Policy Trianing</span></h2>
          </div>

      <div class="columns is-centered">
        <div class="column is-full" style="text-align: center;">
          <img src="media/figures/policy_training.png" alt="Image description" width="80%">
          <br>
          <br>
          <div class="columns is-centered has-text-centered">
            <div class="column is-two-thirds">
              <div class="content has-text-justified interpolation-panel">
                <!-- <div class="content has-text-justified"> -->
                <p style="text-align: center;font-size: 18px"> 
                  We present our policy training pipeline, which uses Behavior Cloning (BC) to train policy π, using optimal expert demonstrations.
                  The image observation (ot) is processed using our HRP representations resulting in a latent vector z. The policy uses z to predict end-effector
                  velocity actions (delta ee-pose/gripper), which are directly executed on the robot during test-time.
                </p>
            </div>
          </div>
        </div>
        <br>
          
        </div>
      </div>


    <!-- Animation. -->
    </div>
    </div>
  </div>
</section>

<section class="hero teaser">
  <div class="container is-max-widescreen">

    <!-- <div class="rows"> -->
      <div class="rows">
        <div class="row is-full-width">
          <div class="column" style="text-align: center;">
          <h2 class="title is-3">Quantitative Results</span></h2>
          </div>

      <div class="columns is-centered">
        <div class="column is-full" style="text-align: center;">
          <img src="media/figures/hrp_cam_results.png" alt="Image description" width="80%">
          <br>
          <br>
          <div class="columns is-centered has-text-centered">
            <div class="column is-two-thirds">
              <div class="content has-text-justified interpolation-panel">
                <!-- <div class="content has-text-justified"> -->
                <p style="text-align: center;font-size: 18px"> 
                  We apply HRP to 6 different baseline representations and plot how it affects performance on average across the toasting, pouring,
and stacking tasks. We evaluate the performance across two distinct cameras in order to test if HRP representation
are robust to view shifts. We find that HRP representations consistently and substantially outperform their vanilla baselines, and that this
effect holds across both the front (left) and ego (right) cameras. In fact, our strongest representation <b>ImageNet + HRP</b> delivers SOTA
performance on both views!
                </p>
            </div>
          </div>
        </div>
        <br>
          
        </div>
      </div>


    <!-- Animation. -->
    </div>
    </div>
  </div>
</section>

<section class="hero teaser">
  <div class="container is-max-widescreen">

    <!-- <div class="rows"> -->
      <div class="rows">
        <div class="row is-full-width">
          <div class="column" style="text-align: center;">
          <h2 class="title is-3">Ablations</span></h2>
          </div>

      <div class="columns is-centered">
        <div class="column is-full" style="text-align: center;">
          <img src="media/figures/hrp_albation_results.png" alt="Image description" width="80%">
          <br>
          <br>
          <div class="columns is-centered has-text-centered">
            <div class="column is-two-thirds">
              <div class="content has-text-justified interpolation-panel">
                <!-- <div class="content has-text-justified"> -->
                <p style="text-align: center;font-size: 18px"> 
                  We ablate HRP (full fine-tuning) to the 6 baseline representations, comparing their average performance versus standard HRP representations on toasting, pouring, and stacking tasks. LayerNorm only fine-tuning is almost always superior.
                  We also drop each of the 3 losses in HRP to compare the ablated method’s average performance against full HRP representations across the toasting, pouring, and stacking tasks. This experiment is conducted on the Ego4D, ImageNet, and VC-1 base models. Object and hand losses are found to be critical for good performance, while the contact loss significantly impacts only the Ego4D base model.                  
                </p>
            </div>
          </div>
        </div>
        <br>
          
        </div>
      </div>


    <!-- Animation. -->
    </div>
    </div>
  </div>
</section>

<footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column">
        <div class="content has-text-centered">
          <p>
            Website template borrowed from <a href="https://github.com/nerfies/nerfies.github.io">NeRFies</a> and <a href="https://peract.github.io/">PerAct</a> and <a href="https://voxposer.github.io">VoxPoser</a>. 
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>

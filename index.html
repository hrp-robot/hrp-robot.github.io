<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="HRP: Human Affordances for Robotic Pre-Training">
  <meta name="keywords" content="Robotic Manipulation">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>HRP</title>

  <script async src="https://www.googletagmanager.com/gtag/js?id=G-FV4ZJ9PVSV"></script>  
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'G-FV4ZJ9PVSV');
  </script>

  <script>

    function updateInteractive() {
      var task = document.getElementById("interative-menu").value;

      console.log("interactive", task)

      var video = document.getElementById("interactive-video1");
      video.src = "assets/h3_vids/goal" + task + ".mp4";
      video.play();

      var video = document.getElementById("interactive-video2");
      video.src = "assets/h3_vids/sketch" + task + ".mp4";
      video.play();

    }

    function updateInteractiveSemantic() {
      var task = document.getElementById("interative-semantic-menu").value;

      console.log("interactive", task)

      var video = document.getElementById("interactive-video-semantic-1");
      video.src = "assets/h4_vids/" + task + "/semantic.mp4";
      video.play();

      var video = document.getElementById("interactive-video-semantic-2");
      video.src = "assets/h4_vids/" + task + "/spatial.mp4";
      video.play();

    }



  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body onload="updateInteractive();">

<section class="hero">
  <div class="hero-body">
    <div class="container is-fullhd">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">HRP: Human Affordances for Robotic Pre-Training</h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a target="_blank" href="http://mohanksriram.com/">Mohan Kumar Srirama,</a>
              <a target="_blank" href="https://sudeepdasari.github.io/">Sudeep Dasari,</a><sup>*</sup>
              <a target="_blank" href="https://shikharbahl.github.io/">Shikhar Bahl,</a><sup>*</sup>
              <a target="_blank" href="http://www.cs.cmu.edu/~abhinavg/">Abhinav Gupta</a><sup>*</sup>
            </span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup><font size="-0.4">*</sup>Equal advising</font></span><br>
            <!-- <span class="author-block"><sup></sup>Carnegie Mellon University</span> -->
          </div>

          <img src="./assets/logos/cmu.png" width="8%" style="margin-right: 35px;"></img>


          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <span class="link-block">
                <a target="_self" href="./additional.html"
                   class="external-link button is-normal is-rounded is-dark">
                  <span>Additional Results</span>
                </a>
              </span>
              <!-- PDF Link. -->
              <span class="link-block">
                <a target="_blank" href="https://arxiv.org/abs/2403.02709"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
            </div>
          </div>

        </div>
      </div>
    </div>
  </div>
</section>


<section class="hero teaser">
  <div class="container is-fullhd">
    <div class="hero-body">
      <div class="container">
        <div class="columns is-vcentered  is-centered">
          <video id="teaser" autoplay muted loop height="70%" width="70%">
            <source src="./media/videos/hrp_task_teaser.mp4"
                    type="video/mp4">
          </video>
          <!-- <img src="./media/videos/hrp_task_teaser.mp4" style="width: 60%; height: auto;"/> -->
          </br>
        </div>
      </div>
    </div>
  </div>
</section>


<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            In order to generalize to various tasks in the wild,
robotic agents will need a suitable representation (i.e., vision
network) that enables the robot to predict optimal actions
given high dimensional vision inputs. However, learning such
a representation requires an extreme amount of diverse training
data, which is prohibitively expensive to collect on a real robot.
How can we overcome this problem? Instead of collecting more
robot data, this paper proposes using internet-scale, human
videos to extract “affordances,” both at the environment and
agent level, and distill them into a pre-trained representation.
We present a simple framework for pre-training representations
on hand, object, and contact “affordance labels” that highlight
relevant objects in images and how to interact with them. These
affordances are automatically extracted from human video data
(with the help of off-the-shelf computer vision modules) and
used to fine-tune existing representations. Our approach can
efficiently fine-tune any existing representation, and results in
models with stronger downstream robotic performance across the
board. We experimentally demonstrate (using 3000+ robot trials)
that this affordance pre-training scheme boosts performance
by a minimum of 15% on 5 real-world tasks, which consider
three diverse robot morphologies (including a dexterous hand).
Unlike prior works in the space, these representations improve
performance across 3 different camera views. Quantitatively, we
find that our approach leads to higher levels of generalization in
out-of-distribution settings. For code, weights, and data check: https://www.cs.cmu.edu/~data4robotics/hrp/index.html
          </p>
        </div>
      </div>
    </div>
    <br>
    <br>
    <!--/ Abstract. -->

  </div>

</section>


<section class="hero teaser">
  <div class="container is-fullhd">
    <div class="hero-body">
      <div class="container">
        <div class="columns is-vcentered  is-centered">
          <video id="teaser" autoplay muted loop height="70%" width="70%">
            <source src="./media/videos/teaser.mp4"
                    type="video/mp4">
          </video>
          </br>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="hero teaser">
  <div class="container is-max-widescreen">

    <div class="rows">


    <!-- Animation. -->
    <div class="rows is-centered ">
      <div class="row is-full-width">
        <h2 class="title is-3">Method</span></h2>
        <img src="media/figures/method.png" class="interpolation-image" style="width: 100%; height: auto;"/>
      </br>
      <p class="content has-text-justified">
        HRP fine-tunes a pre-trained encoder to predict three classes of human affordance labels via L2 regression. Specifically,
the network must predict future contact points, human hand poses, and the target object given an input frame from the
video stream. These affordance labels are mined autonomously from a human video dataset [33] using off-the-shelf vision
detectors [81]. HRP representations are then fine-tuned to solve downstream manipulation tasks via behavior cloning.
      </p>

      <h2 class="title is-3">Policy Training</span></h2>
        </br>
          <img src="media/figures/policy_training.png" class="interpolation-image" style="width: 100%; height: auto;"/>
        </br>
        </br>
          <p class="content has-text-justified">
            We present our policy training pipeline, which uses Behavior Cloning (BC) to train policy π, using optimal expert demonstrations.
The image observation (ot) is processed using our HRP representations resulting in a latent vector z. The policy uses z to predict end-effector
velocity actions (delta ee-pose/gripper), which are directly executed on the robot during test-time.
          </p>
        </br>
        </br>

        </div>
      </div>
    </div>
    </div>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Results</h2>
      </div>
    </div>
  </div>
  <div class="container is-fullhd">
    <div class="hero-body">
      <div class="container">
        <h2 class="subtitle has-text-centered">
        HRP is an <b>affordance based</b> visual pre-training method that <br>
        </h2>
        <h2 class="body">

        <b>(1)</b> Allows us to learn <b>actionable representations</b> from human videos! <br>
        <b>(2)</b> Improves performance for almost any pre-trained model (CLIP, DINO, Ego4D, ImageNet, etc)<br>
        <b>(3)</b> Robust <b>across multiple tasks (including dexterous) and unique camera views</b><br>
        </h2>

      </div>
    </div>
  </div>
</section>


<section class="hero teaser">
  <div class="container is-max-widescreen">

    <div class="rows">
    <!-- Animation. -->
    <div class="rows is-centered ">
      <div class="row is-full-width is-centered">
        <h2 class="title is-3">Quantitative Results</span></h2>
        <p class="content has-text-justified">
          We apply HRP to 6 different baseline representations and plot how it affects performance on average across the toasting, pouring,
and stacking tasks. We evaluate the performance across two distinct cameras in order to test if HRP representation
are robust to view shifts. We find that HRP representations consistently and substantially outperform their vanilla baselines, and that this
effect holds across both the front (left) and ego (right) cameras. In fact, our strongest representation <b>ImageNet + HRP</b> delivers SOTA
performance on both views!
        <br>
        <br>
        <h3 class="title is-5">Finetuning Performance</h3>
        </p>
        <img src="media/figures/hrp_cam_results.png" style="width: 100%; height: auto;"/>
        <p class="content has-text-justified">
        <br>
        <br>
        <h3 class="title is-5">Ablations</h3>
        <p class="content has-text-justified">
          We ablate HRP (full fine-tuning) to the 6 baseline representations, comparing their average performance versus standard HRP representations on toasting, pouring, and stacking tasks. LayerNorm only fine-tuning is almost always superior.

We also drop each of the 3 losses in HRP to compare the ablated method’s average performance against full HRP representations across the toasting, pouring, and stacking tasks. This experiment is conducted on the Ego4D, ImageNet, and VC-1 base models. Object and hand losses are found to be critical for good performance, while the contact loss significantly impacts only the Ego4D base model.
        </p>
        </p>
      </div>
    </div>
    </div>
    <div class="columns is-centered has-text-centered">
        <img src="media/figures/hrp_albation_results.png" style="width: 100%; height: auto;"/>
    </div>
   </div>
</section>

<section class="section">
  <div class="container is-max-widescreen">

    <div class="rows">
      <h2 class="title is-3">Failure Modes</h2>
      <p class="content has-text-justified">
      RT-Sketch's main failure modes are imprecision and moving the wrong object. We see the first failure mode typically when RT-Sketch positions an object correctly but fails to reorient it (common in the upright task). The second failure mode is most apparent in the case of visual distractor objects, where RT-Sketch mistakenly picks up the wrong object and puts in the appropriate place. We posit that both of these failures are due to RT-Sketch being trained on GAN-generated sketches, which occasionally do not preserve geometric details well, leading the policy to not pay close attention to objects or object orientations.
      </p>
        <div class="rows is-centered">
            <h3 class="title is-4">Imprecision</h3>
          <div class="columns is-centered has-text-centered">
            <div class="column"><p>Coke can moved to correct location, but not upright</p></div>
            <div class="column"><p>Pepsi can moved to correct location, but not upright</p></div>
          </div>
          <div class="columns is-centered has-text-centered">
            <video id="imprecise1" autoplay muted loop height="50%" width="50%">
              <source src="assets/imprecise1.mp4"
                      type="video/mp4">
            </video>
            <video id="imprecise2" autoplay muted loop height="50%" width="50%">
              <source src="assets/imprecise2.mp4"
                      type="video/mp4">
            </video>
          </div>
          <br>
            <h3 class="title is-4">Wrong Object</h3>
          <div class="columns is-centered has-text-centered">
            <div class="column"><p>Apple moved instead of coke can</p></div>
            <div class="column"><p>Coke can moved instead of fruit</p></div>
          </div>
          <div class="columns is-centered has-text-centered">
            <video id="wrongobject1" autoplay muted loop height="50%" width="50%">
              <source src="assets/wrong_obj1.mp4"
                      type="video/mp4">
            </video>
            <video id="wrongobject2" autoplay muted loop height="50%" width="50%">
              <source src="assets/wrong_obj2.mp4"
                      type="video/mp4">
            </video>
          </div>
    </div>
  </div>
</section>


<footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column">
        <div class="content has-text-centered">
          <p>
            Website template borrowed from <a href="https://github.com/nerfies/nerfies.github.io">NeRFies</a> and <a href="https://peract.github.io/">PerAct</a> and <a href="https://voxposer.github.io">VoxPoser</a>. 
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>
